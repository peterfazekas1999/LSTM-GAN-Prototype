{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train and test data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "#if opened as a kaggle notebook, the data should be loaded correctly\n",
    "#otherwise change the code accordingly\n",
    "\n",
    "print('Loading train and test data...')\n",
    "#df=pd.read_csv('../input/kepler-labelled-time-series-data/exoTrain.csv')\n",
    "df=pd.read_csv('exoTrain.csv')\n",
    "#print(df1)\n",
    "#df2=pd.read_csv('../input/kepler-labelled-time-series-data/exoTest.csv')\n",
    "df2=pd.read_csv('exoTest.csv')\n",
    "#print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_data)\n",
    "test_data=np.array(df2,dtype=np.float32)\n",
    "#print(test_data)\n",
    "\n",
    "\n",
    "Y=df.loc[:,df.columns==\"LABEL\"]\n",
    "X=df.loc[:,df.columns!=\"LABEL\"]\n",
    "ytest=test_data[:,0]\n",
    "Xtest=test_data[:,1:]\n",
    "# print(ytrain,'\\n',Xtrain)\n",
    "# print(ytest,'\\n',Xtest)\n",
    "\n",
    "m=1 # A chosen exoplanet host star's index for plott\n",
    "n=100 # A chosen non-exoplanet host star's index\n",
    "\n",
    "#print('Shape of Xtrain:',np.shape(Xtrain),'\\nShape of ytrain:',np.shape(ytrain))\n",
    "\n",
    "Xtrain =X\n",
    "Y =Y.to_numpy()\n",
    "\n",
    "# plt.plot(X.iloc[m],'r')\n",
    "# plt.title('Light intensity vs time (for an exoplanet star)')\n",
    "# plt.xlabel('Time index')\n",
    "# plt.ylabel('Light intensity')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier tranform, de-noising, normalising and scaling of the lightcurves.                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently commented out as I am feeding the LSTM-GAN the original lightcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft,ifft\n",
    "\n",
    "print('Applying Fourier Transform...')\n",
    "\n",
    "Xtrain=Xtrain.to_numpy()\n",
    "plt.figure(5)\n",
    "plt.plot(Xtrain[1])\n",
    "#Xtrain=np.abs(fft(Xtrain,n=len(Xtrain[m]),axis=1))\n",
    "\n",
    "\n",
    "print(Xtrain,Xtrain.shape)\n",
    "\n",
    "Xtrain=Xtrain[:,:1+int((len(Xtrain[0])-1)/2)]\n",
    "\n",
    "\n",
    "\n",
    "Xtest=Xtest[:,:1+int((len(Xtest[0])-1)/2)]\n",
    "plt.figure(1)\n",
    "save_train_m = Xtrain[m]\n",
    "plt.plot(Xtrain[m],'r')\n",
    "plt.title('After FFT (for an exoplanet star)')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Feature value')\n",
    "plt.show()\n",
    "\n",
    "#### Normalizing\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "print('Normalizing...')\n",
    "#Xtrain=normalize(Xtrain)\n",
    "Xtest=normalize(Xtest)\n",
    "plt.figure(2)\n",
    "plt.plot(Xtrain[m],'r')\n",
    "plt.title('After FFT,Normalization (for an exoplanet star)')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Feature value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Applying Gaussian Filter\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "print('Applying Gaussian filter...')\n",
    "#Xtrain=ndimage.filters.gaussian_filter(Xtrain,sigma=10)\n",
    "Xtest=ndimage.filters.gaussian_filter(Xtest,sigma=10)\n",
    "plt.figure(3)\n",
    "plt.plot(Xtrain[m],'r')\n",
    "plt.title('After FFT,Normalization and Gaussian filtering (for an exoplanet star)')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Feature value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Scaling down the data\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print('Applying MinMaxScaler...')\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "#Xtrain=scaler.fit_transform(Xtrain)\n",
    "Xtest=scaler.fit_transform(Xtest)\n",
    "plt.figure(4)\n",
    "plt.plot(Xtrain[m],'r')\n",
    "plt.title('After FFT,Normalization, Gaussian filtering and scaling (for an exoplanet star)')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Feature value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN MODEL. Run this cell to train the model and generate exo planet lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model deployment\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,SimpleRNN\n",
    "from keras.layers import Dropout,GRU,Flatten,Input\n",
    "from keras.layers import TimeDistributed,Reshape\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential, load_model\n",
    "from tqdm import tqdm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam,Adamax,Nadam\n",
    "adam = Adam(learning_rate = 3E-2)\n",
    "print(Xtrain.shape)\n",
    "print(Y.shape)\n",
    "#Xtrain = np.reshape(Xtrain,(Xtrain.shape[0],Xtrain.shape[1],1))\n",
    "print(Y)\n",
    "#for i in range(len(Y)):\n",
    " #   if(Y[i]==2):\n",
    "  #      Y[i] = -1\n",
    "print(Y)\n",
    "\n",
    "\n",
    "#the generator takes in 100 random points, its\n",
    "# a fully connected NN and it outputs 1500 points which is about\n",
    "#the length of a lightcurve\n",
    "def create_generator():\n",
    "    generator=Sequential()\n",
    "    generator.add(Dense(units=512,input_dim=100))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(units=1024))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(units=1024))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(units=Xtrain.shape[1]))\n",
    "    \n",
    "    generator.add(Reshape((Xtrain.shape[1],1)))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "    return generator\n",
    "g = create_generator()\n",
    "g.summary()\n",
    "\n",
    "#The discriminator takes in the output generated by the generator function\n",
    "#  which produces the lightcurve and the discriminator then outputs 1 or -1 \n",
    "# checking whether it is an exo-planet or not\n",
    "def create_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units =256,input_shape=(Xtrain.shape[1],1),return_sequences = True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(units =256,return_sequences = True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(units =128,return_sequences = False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units =1,activation = \"tanh\"))\n",
    "    model.compile(loss='binary_crossentropy',optimizer=\"rmsprop\",metrics = [\"accuracy\"])\n",
    "    return model\n",
    "d =create_discriminator()\n",
    "d.summary()\n",
    "\n",
    "#This merges the 2 models together to make the GAN\n",
    "def create_gan(discriminator, generator):\n",
    "    discriminator.trainable=False\n",
    "    gan_input = Input(shape=(100,))\n",
    "    x = generator(gan_input)\n",
    "\n",
    "    gan_output= discriminator(x)\n",
    "    gan= Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "    return gan\n",
    "\n",
    "gan = create_gan(d,g)\n",
    "gan.summary()\n",
    "\n",
    "\n",
    "\n",
    "#This is how a GAN is supposed to be trained for handwritten digit recognition\n",
    "#I have changed the code slightly so that the dimensions are correct for\n",
    "# the LSTM block during training\n",
    "def training(epochs=1, batch_size=128):\n",
    "    \n",
    "    #Loading the data\n",
    "    X_train, y_train = Xtrain,Y\n",
    "    batch_count = X_train.shape[0] / batch_size\n",
    "    X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "    # Creating GAN\n",
    "    generator= create_generator()\n",
    "    discriminator= create_discriminator()\n",
    "    gan = create_gan(discriminator, generator)\n",
    "    \n",
    "    for e in range(1,epochs+1 ):\n",
    "        print(\"Epoch %d\" %e)\n",
    "        for _ in tqdm(range(batch_size)):\n",
    "        #generate  random noise as an input  to  initialize the  generator\n",
    "            noise= np.random.normal(0,1, [batch_size, 100])\n",
    "            \n",
    "            # Generate fake MNIST images from noised input\n",
    "            generated_images = generator.predict(noise)\n",
    "            \n",
    "            # Get a random set of  real images\n",
    "            image_batch =X_train[np.random.randint(low=0,high=X_train.shape[0],size=batch_size)]\n",
    "            \n",
    "            #Construct different batches of  real and fake data \n",
    "            X= np.concatenate([image_batch, generated_images])\n",
    "            X = np.reshape(X,(X.shape[0],X.shape[1],1))\n",
    "            # Labels for generated and real data\n",
    "            y_dis=np.zeros(2*batch_size)\n",
    "            y_dis[:batch_size]=0.9\n",
    "            \n",
    "            #Pre train discriminator on  fake and real data  before starting the gan. \n",
    "            discriminator.trainable=True\n",
    "            discriminator.train_on_batch(X, y_dis)\n",
    "            \n",
    "            #Tricking the noised input of the Generator as real data\n",
    "            noise= np.random.normal(0,1, [batch_size, 100])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            \n",
    "            # During the training of gan, \n",
    "            # the weights of discriminator should be fixed. \n",
    "            #We can enforce that by setting the trainable flag\n",
    "            discriminator.trainable=False\n",
    "            \n",
    "            #training  the GAN by alternating the training of the Discriminator \n",
    "            #and training the chained GAN model with Discriminatorâ€™s weights freezed.\n",
    "            gan.train_on_batch(noise, y_gen)\n",
    "            \n",
    "        if e == 1 or e % 5 == 0:\n",
    "            print(generated_images.shape)\n",
    "        \n",
    "            for i in range(0,batch_size):\n",
    "                plt.figure()\n",
    "                plt.plot(generated_images[i])\n",
    "            plt.show()\n",
    "        if e==epochs:\n",
    "            for i in range(0,batch_size,2):\n",
    "                plt.figure()\n",
    "                plt.plot(generated_images[i])\n",
    "training(300,10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generic LSTM Model, with and without time distributed layer, this is a different architecture from a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model deployment\n",
    "\n",
    "# print(Xtrain.shape)\n",
    "# Xtrain = np.reshape(Xtrain,(Xtrain.shape[0],Xtrain.shape[1],1))\n",
    "# print(Xtrain.shape)\n",
    "# print(Xtrain)\n",
    "X_tester = Xtrain\n",
    "xtrain_t = []\n",
    "ytrain_t = []\n",
    "graph = []\n",
    "for j in range(1):\n",
    "    for i in range(1,len(X_tester[0])):\n",
    "        graph.append(X_tester[j][i])\n",
    "for j in range(1,3):\n",
    "    for i in range(100,len(X_tester[0])):\n",
    "        xtrain_t.append(X_tester[j][i-100:i])\n",
    "        ytrain_t.append(X_tester[j][i])\n",
    "xtrain_t = np.array(xtrain_t)\n",
    "ytrain_t = np.array(ytrain_t)\n",
    "print(xtrain_t.shape)\n",
    "# print(xtrain_t)\n",
    "xtrain_t = np.reshape(xtrain_t,(xtrain_t.shape[0],xtrain_t.shape[1],1))\n",
    "# ytrain_t = np.reshape(ytrain_t,(ytrain_t.shape[0],1))\n",
    "#print(xtrain_t)\n",
    "print(xtrain_t.shape)\n",
    "    \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,SimpleRNN\n",
    "from keras.layers import Dropout,GRU,Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential, load_model\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam,Adamax,Nadam\n",
    "adam = Adam(learning_rate = 3E-2)\n",
    "model = Sequential()\n",
    "model.add(LSTM(units =500,input_shape=(xtrain_t.shape[1],1),return_sequences = True))\n",
    "model.add(Dense(units =1))\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"rmsprop\",metrics = [\"accuracy\"])\n",
    "model.summary()\n",
    "model.fit(xtrain_t,ytrain_t,epochs =10,batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timeDistributed():\n",
    "    temp1 = model.predict(xtrain_t[:1])\n",
    "    print(temp1.shape)\n",
    "    print(xtrain_t[:1].shape)\n",
    "   \n",
    "   \n",
    "    \n",
    "    limit = 3\n",
    "    trial =[]\n",
    "    temp1 = xtrain_t[:1]\n",
    "    for i in range(limit):\n",
    "        temp1 = model.predict(temp1)\n",
    "        temp1 =np.reshape(temp1,(temp1.shape[1]))\n",
    "        for j in temp1:\n",
    "            trial.append(j)\n",
    "        temp1 = np.reshape(temp1,(1,temp1.shape[0],1))\n",
    "    #plotting the results\n",
    "    abs_trial = []\n",
    "    for i in range(len(trial)):\n",
    "        abs_trial.append(abs(trial[i]))\n",
    "    plt.figure()  \n",
    "    \n",
    "    plt.plot(trial)\n",
    "    plt.xlim(-30,1000)\n",
    "    plt.title('Predicted lightcurve with FFT,Normalization, Gaussian filtering and scaling')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Feature value')\n",
    "    \n",
    "    #plotting the actual FFT lightcurve\n",
    "    plt.figure()\n",
    "    plt.plot(graph,color = \"r\")\n",
    "    plt.xlim(-30,1000)\n",
    "    plt.title('After FFT,Normalization, Gaussian filtering and scaling (for an exoplanet star)')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Feature value')\n",
    "    #plt.plot(np.reshape(xtrain_t[:1],(xtrain_t[:1].shape[1])),color = \"r\")\n",
    "    #plt.plot(np.reshape(temp1,(temp1.shape[1])),color = \"b\")\n",
    "    plt.show()\n",
    "    return trial\n",
    "    \n",
    "trial =timeDistributed()\n",
    "\n",
    "\n",
    "def nonTimeDistributed():\n",
    "    limit =1\n",
    "    trial = []\n",
    "    trial_t = xtrain_t[:1]\n",
    "\n",
    "    for i in range(limit):\n",
    "        yt = model.predict(trial_t)\n",
    "        trial.append([yt[0][0]])\n",
    "        trial_t = np.reshape(trial_t,(trial_t.shape[1]))\n",
    "        trial_t =np.append(trial_t,yt[0][0])\n",
    "        trial_t = trial_t[1:]\n",
    "        trial_t = np.reshape(trial_t,(1,trial_t.shape[0],1))\n",
    "\n",
    "    plt.plot(trial)\n",
    "\n",
    "\n",
    "    xtester = model.predict(xtrain_t)\n",
    "    print(trial)\n",
    "    print(xtester)\n",
    "\n",
    "    plt.figure(5)\n",
    "    plt.plot(xtester[:1000],color =\"r\")\n",
    "    plt.figure(6)\n",
    "    plt.plot(ytrain_t[:1000],color = \"b\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_ns = np.asarray(trial)\n",
    "graph_ns = np.asarray(graph)\n",
    "noise = np.random.normal(0,0.1,trial_ns.shape)\n",
    "\n",
    "print(trial_ns.shape)\n",
    "trial_ns = trial_ns + noise\n",
    "noise = np.random.normal(0.02,0.09,graph_ns.shape)\n",
    "graph_ns = graph_ns+noise\n",
    "plt.figure()\n",
    "plt.plot(trial_ns)\n",
    "plt.figure()\n",
    "plt.plot(graph_ns)\n",
    "plt.figure()\n",
    "plt.plot(save_train_m,color = \"r\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ns_ifft = ifft(trial_ns)\n",
    "plt.plot(trial_ns_ifft)\n",
    "plt.xlim(100,1000)\n",
    "plt.ylim(-0.015,0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
