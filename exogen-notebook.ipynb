{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GAN MODEL ONLY, Run Cells below to train GAN generate transit Light Curves"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nnp.random.seed(1)\n\nprint('Loading train and test data...')\ndf=pd.read_csv('../input/kepler-labelled-time-series-data/exoTrain.csv')\n#print(df1)\ndf2=pd.read_csv('../input/kepler-labelled-time-series-data/exoTest.csv')\n#print(df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(train_data)\n\n\nY=df.loc[df[\"LABEL\"]==2]\nY=df.loc[:,df.columns==\"LABEL\"]\nX=df.loc[df[\"LABEL\"]==2]        #train the GAN on existing exoplanet lightcurves\nX=df.loc[:,df.columns!=\"LABEL\"]\n\nm=1 # A chosen exoplanet host star's index for plott\nn=100 # A chosen non-exoplanet host star's index\n\nXtrain =X\nY =Y.to_numpy()\nXtrain=Xtrain.to_numpy()\nplt.figure(5)\nplt.plot(Xtrain[1])\n\n\nXtrain=Xtrain[:,:1+int((len(Xtrain[0])-1)/2)]  #trim the Xtrain data down to about 1600 datapoints instead of 3000+","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Xtrain.shape)\nprint(Y.shape)\nXtrain = np.reshape(Xtrain,(Xtrain.shape[0],1,Xtrain.shape[1]))\nprint(Xtrain.shape)\nprint(Y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential,Model\nfrom keras.layers import Dense\nfrom keras.layers import LSTM,SimpleRNN\nfrom keras.layers import Dropout,GRU,Flatten,Input\nfrom keras.layers import TimeDistributed,Reshape\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential, load_model\nfrom tqdm import tqdm\nfrom keras.layers.advanced_activations import LeakyReLU\nimport tensorflow as tf\nfrom keras.optimizers import Adam,Adamax,Nadam\nadam = Adam(learning_rate = 3E-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#the generator for the GAN\ndef create_generator():\n    generator=Sequential()\n    generator.add(Dense(units=512,input_dim=100))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=1024))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=1024))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=Xtrain.shape[2]))\n    \n    generator.add(Reshape((1,Xtrain.shape[2])))\n    \n    generator.compile(loss='binary_crossentropy', optimizer=adam)\n    return generator\ng = create_generator()\ng.summary()\n\n# the discriminator for the GAN\ndef create_discriminator():\n    model = Sequential()\n    model.add(LSTM(units =256,input_shape=(1,Xtrain.shape[2]),return_sequences = True))\n    model.add(Dropout(0.3))\n    model.add(LSTM(units =256,return_sequences = True))\n    model.add(Dropout(0.3))\n    model.add(LSTM(units =128,return_sequences = False))\n    model.add(Dropout(0.3))\n    model.add(Dense(units =1,activation = \"sigmoid\"))\n    model.compile(loss='binary_crossentropy',optimizer=\"rmsprop\",metrics = [\"accuracy\"])\n    return model\nd =create_discriminator()\nd.summary()\n\ndef create_gan(discriminator, generator):\n    discriminator.trainable=False\n    gan_input = Input(shape=(100,))\n    x = generator(gan_input)\n    \n    gan_output= discriminator(x)\n    gan= Model(inputs=gan_input, outputs=gan_output)\n    gan.compile(loss='binary_crossentropy', optimizer='rmsprop')\n    return gan\n\ngan = create_gan(d,g)\ngan.summary()\n\n\ndef training(epochs=1, batch_size=128):\n    \n    #Loading the data\n    X_train, y_train = Xtrain,Y\n    batch_count = X_train.shape[0] / batch_size\n    X_train = np.reshape(X_train,(X_train.shape[0],1,X_train.shape[2]))\n    # Creating GAN\n    generator= create_generator()\n    discriminator= create_discriminator()\n    gan = create_gan(discriminator, generator)\n    \n    for e in range(1,epochs+1 ):\n        print(\"Epoch %d\" %e)\n        for _ in tqdm(range(batch_size)):\n        #generate  random noise as an input  to  initialize the  generator\n            noise= np.random.normal(0,1, [batch_size, 100])\n            \n            # Generate fake MNIST images from noised input\n            generated_images = generator.predict(noise)\n            \n            # Get a random set of  real images\n            image_batch =X_train[np.random.randint(low=0,high=X_train.shape[0],size=batch_size)]\n            \n            #Construct different batches of  real and fake data \n            X= np.concatenate([image_batch, generated_images])\n            X = np.reshape(X,(X.shape[0],1,X_train.shape[2]))\n            # Labels for generated and real data\n            y_dis=np.zeros(2*batch_size)\n            y_dis[:batch_size]=0.9\n            \n            #Pre train discriminator on  fake and real data  before starting the gan. \n            discriminator.trainable=True\n            discriminator.train_on_batch(X, y_dis)\n            \n            #Tricking the noised input of the Generator as real data\n            noise= np.random.normal(0,1, [batch_size, 100])\n            y_gen = np.ones(batch_size)\n            \n            # During the training of gan, \n            # the weights of discriminator should be fixed. \n            #We can enforce that by setting the trainable flag\n            discriminator.trainable=False\n            \n            #training  the GAN by alternating the training of the Discriminator \n            #and training the chained GAN model with Discriminatorâ€™s weights freezed.\n            gan.train_on_batch(noise, y_gen)\n            \n        if e == 1 or e % 5 == 0:\n            print(generated_images.shape)\n        \n            for i in range(0,batch_size):\n                plt.figure()\n                plt.plot(generated_images[i][0])\n            plt.show()\n        if e==epochs:\n            for i in range(0,batch_size,2):\n                plt.figure()\n                plt.plot(generated_images[i])\n            generator.save(\"generator.h5\")\n            generator.save_weights(\"generator.h5\")\n            gan.save(\"gan\")\n            discriminator.save(\"discriminator.h5\")\ntraining(200,10)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"EXPORT THE MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = load_model(\"gan\")\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n#Here make sure the layer names match the imported GAN Models layers\nnew_model = Model(model.get_layer(\"sequential_7\").input,model.get_layer(\"sequential_7\").output)\nnew_model.load_weights(\"generator_weights2.h5\")\n\nprint(new_model.summary())\n\n\n\nbatch_size = 5\nnoise = np.random.normal(0,1,[batch_size,100])\ntest = new_model.predict(noise)\ndef printer():\n    for i in range(0,batch_size):\n        plt.figure()\n        \n        plt.plot(test[i][0])\n    plt.show()\nprinter()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CLASSIFIER MODEL, Run cells below to train the LSTM and make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/kepler-labelled-time-series-data/exoTrain.csv')\nY=df.loc[:,df.columns==\"LABEL\"]\nX=df.loc[:,df.columns!=\"LABEL\"]\nXtrain =X\nXtrain = Xtrain.to_numpy()\nY =Y.to_numpy()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.fftpack import fft,ifft\n#standard data preprocessing before feeding it to the LSTM \ndef preprocess(Xtrain):\n    print('Applying Fourier Transform...')\n\n\n    plt.figure(5)\n    plt.plot(Xtrain[1])\n    Xtrain=np.abs(fft(Xtrain,n=len(Xtrain[m]),axis=1))\n\n\n    print(Xtrain,Xtrain.shape)\n\n    Xtrain=Xtrain[:,:1+int((len(Xtrain[0])-1)/2)]\n\n    plt.figure(1)\n    save_train_m = Xtrain[m]\n    plt.plot(Xtrain[m],'r')\n    plt.title('After FFT (for an exoplanet star)')\n    plt.xlabel('Frequency')\n    plt.ylabel('Feature value')\n    plt.show()\n\n    #### Normalizing\n\n    from sklearn.preprocessing import normalize\n\n    print('Normalizing...')\n    Xtrain=normalize(Xtrain)\n   \n    plt.figure(2)\n    plt.plot(Xtrain[m],'r')\n    plt.title('After FFT,Normalization (for an exoplanet star)')\n    plt.xlabel('Frequency')\n    plt.ylabel('Feature value')\n    plt.show()\n\n\n\n\n    #### Applying Gaussian Filter\n\n    from scipy import ndimage\n\n    print('Applying Gaussian filter...')\n    Xtrain=ndimage.filters.gaussian_filter(Xtrain,sigma=10)\n    \n    plt.figure(3)\n    plt.plot(Xtrain[m],'r')\n    plt.title('After FFT,Normalization and Gaussian filtering (for an exoplanet star)')\n    plt.xlabel('Frequency')\n    plt.ylabel('Feature value')\n    plt.show()\n\n\n\n\n    #### Scaling down the data\n\n    from sklearn.preprocessing import MinMaxScaler\n\n    print('Applying MinMaxScaler...')\n    scaler=MinMaxScaler(feature_range=(0,1))\n    Xtrain=scaler.fit_transform(Xtrain)\n    \n    plt.figure(4)\n    plt.plot(Xtrain[m],'r')\n    plt.title('After FFT,Normalization, Gaussian filtering and scaling (for an exoplanet star)')\n    plt.xlabel('Frequency')\n    plt.ylabel('Feature value')\n    plt.show()\n    return Xtrain\nXtrain = preprocess(Xtrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Xtrain.shape)\nprint(Y.shape)\n\nXtrain = np.reshape(Xtrain,(Xtrain.shape[0],1,Xtrain.shape[1]))\nprint(Xtrain.shape)\nprint(Y)\n\nfor i in range(len(Y)):\n    if Y[i]==1:\n        Y[i]=0\nfor i in range(len(Y)):\n    if Y[i]==2:\n        Y[i]=1\nprint(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nClassifier = Sequential()\nClassifier.add(LSTM(units =256,input_shape=(1,Xtrain.shape[2]),return_sequences = True))\nClassifier.add(Dropout(0.3))\nClassifier.add(LSTM(units =256,return_sequences = True))\nClassifier.add(Dropout(0.3))\nClassifier.add(LSTM(units =128,return_sequences = False))\nClassifier.add(Dropout(0.3))\nClassifier.add(Dense(units =1,activation = \"sigmoid\"))\nClassifier.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics = [\"accuracy\"])\n\nClassifier.fit(Xtrain,Y,epochs = 10)\n\nClassifier.save(\"classifier.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#quick sanity check after the model is trained\nprint(Xtrain[23])\nplt.plot(Xtrain[34][0])\nplt.show()\nprint(Y[23])\n\nprint(Xtrain[0].shape)\ny = Classifier.predict(Xtrain)\nprint(\"prediction: \",y[322])\nprint(\"actual label: \",Y[322])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_GAN(Xtrain):\n    print('Applying Fourier Transform...')\n    Xtrain=np.abs(fft(Xtrain,n=len(Xtrain[m]),axis=1))\n    print(Xtrain,Xtrain.shape)\n\n    #### Normalizing\n    from sklearn.preprocessing import normalize\n    print('Normalizing...')\n    Xtrain=normalize(Xtrain)\n   \n    #### Applying Gaussian Filter\n    from scipy import ndimage\n    print('Applying Gaussian filter...')\n    Xtrain=ndimage.filters.gaussian_filter(Xtrain,sigma=10)\n    \n    \n    #### Scaling down the data\n    from sklearn.preprocessing import MinMaxScaler\n    print('Applying MinMaxScaler...')\n    scaler=MinMaxScaler(feature_range=(0,1))\n    Xtrain=scaler.fit_transform(Xtrain)\n    \n    plt.figure(4)\n    plt.plot(Xtrain[m],'r')\n    plt.show()\n    return Xtrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.shape)\nbatch_size = 5\nnoise = np.random.normal(0,1,[batch_size,100])\ntest = new_model.predict(noise)\nX_val = np.reshape(test,(test.shape[0],test.shape[2]))\nX_val = preprocess_GAN(X_val)\nX_val = np.reshape(X_val,(X_val.shape[0],1,X_val.shape[1]))\nplt.plot(X_val[1][0])\nplt.show()\ny_val = Classifier.predict(X_val)\nprint(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}